{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop((112)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_size=112):\n",
    "    data = []\n",
    "    labels = {}\n",
    "    index = -1\n",
    "    for label in os.listdir('./data/'):\n",
    "        index += 1\n",
    "        labels[label] = index\n",
    "    print(len(labels))\n",
    "    X = []\n",
    "    y = []\n",
    "    for label in labels:\n",
    "        for file in os.listdir(f'./data/{label}/'):\n",
    "            path = f'./data/{label}/{file}'\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img,(img_size,img_size))\n",
    "            data.append([np.array(transform_data(np.array(img))),labels[label]])\n",
    "            X.append(np.array(transform_data(np.array(img))))\n",
    "            y.append(labels[label])\n",
    "    np.random.shuffle(data)\n",
    "    np.random.shuffle(data)\n",
    "    np.random.shuffle(data)\n",
    "    np.random.shuffle(data)\n",
    "    np.random.shuffle(data)\n",
    "    np.save('./data.npy',data)\n",
    "    VAL_SPLIT = 0.25\n",
    "    VAL_SPLIT = len(X)*VAL_SPLIT\n",
    "    VAL_SPLIT = int(VAL_SPLIT)\n",
    "    X_train = X[:-VAL_SPLIT]\n",
    "    y_train = y[:-VAL_SPLIT]\n",
    "    X_test = X[-VAL_SPLIT:]\n",
    "    y_test = y[-VAL_SPLIT:]\n",
    "    X = torch.from_numpy(np.array(X))\n",
    "    y = torch.from_numpy(np.array(y))\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    return X,y,X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/indika/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "X,y,X_train,X_test,y_train,y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32,5)\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.conv2batchnorm = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "        self.fc1 = nn.Linear(128*10*10,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,50)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        preds = F.max_pool2d(self.relu(self.conv1(X)),(2,2))\n",
    "        preds = F.max_pool2d(self.relu(self.conv2batchnorm(self.conv2(preds))),(2,2))\n",
    "        preds = F.max_pool2d(self.relu(self.conv3(preds)),(2,2))\n",
    "        preds = preds.view(-1,128*10*10)\n",
    "        preds = self.relu(self.fc1(preds))\n",
    "        preds = self.relu(self.fc2(preds))\n",
    "        preds = self.relu(self.fc3(preds))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BaseLine().to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "in_f = model.fc.in_features\n",
    "model.fc = nn.Linear(in_f,50)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'Car-Brands-Images-Clf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(criterion,y,model,X):\n",
    "    model.to('cuda')\n",
    "    preds = model(X.view(-1,3,112,112).to('cuda').float())\n",
    "    preds.to('cuda')\n",
    "    loss = criterion(preds,torch.tensor(y,dtype=torch.long).to('cuda'))\n",
    "    loss.backward()\n",
    "    return loss.item()\n",
    "def test(net,X,y):\n",
    "    device = 'cuda'\n",
    "    net.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X)):\n",
    "            real_class = torch.argmax(y[i]).to(device)\n",
    "            net_out = net(X[i].view(-1,3,112,112).to(device).float())\n",
    "            net_out = net_out[0]\n",
    "            predictied_class = torch.argmax(net_out)\n",
    "            if predictied_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    net.train()\n",
    "    net.to('cuda')\n",
    "    return round(correct/total,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=PROJECT_NAME,name='transfer-learning')\n",
    "# for _ in tqdm(range(EPOCHS)):\n",
    "#     for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#         X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#         y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#         model.to(device)\n",
    "#         preds = model(X_batch)\n",
    "#         preds = preds.to(device)\n",
    "#         loss = criterion(preds,y_batch)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         wandb.log({'loss':loss.item(),'val_loss':get_loss(criterion,y_test,model,X_test),'accuracy':test(model,X_train,y_train),'val_accuracy':test(model,X_test,y_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TL vs Custom Model best = TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = round(2.5)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranuga-d\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">models.resnet18</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ranuga-d/Car-Brands-Images-Clf\" target=\"_blank\">https://wandb.ai/ranuga-d/Car-Brands-Images-Clf</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ranuga-d/Car-Brands-Images-Clf/runs/2a2i7wms\" target=\"_blank\">https://wandb.ai/ranuga-d/Car-Brands-Images-Clf/runs/2a2i7wms</a><br/>\n",
       "                Run data is saved locally in <code>/home/indika/Programming/Projects/Python/Artifical-Intelligence/PyTorch/CNN/Car-Brands-Images-Clf/wandb/run-20210522_120845-2a2i7wms</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/108 [00:00<?, ?it/s]\u001b[A/home/indika/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "\n",
      "  1%|          | 1/108 [00:11<20:10, 11.32s/it]\u001b[A\n",
      "  2%|▏         | 2/108 [00:22<19:40, 11.13s/it]\u001b[A\n",
      "  3%|▎         | 3/108 [00:33<19:47, 11.31s/it]\u001b[A\n",
      "  4%|▎         | 4/108 [00:44<19:25, 11.21s/it]\u001b[A\n",
      "  5%|▍         | 5/108 [00:55<19:05, 11.13s/it]\u001b[A\n",
      "  6%|▌         | 6/108 [01:06<18:47, 11.05s/it]\u001b[A\n",
      "  6%|▋         | 7/108 [01:17<18:35, 11.04s/it]\u001b[A\n",
      "  7%|▋         | 8/108 [01:28<18:22, 11.02s/it]\u001b[A\n",
      "  8%|▊         | 9/108 [01:39<18:09, 11.01s/it]\u001b[A\n",
      "  9%|▉         | 10/108 [01:50<17:59, 11.01s/it]\u001b[A\n",
      " 10%|█         | 11/108 [02:01<17:45, 10.99s/it]\u001b[A\n",
      " 11%|█         | 12/108 [02:12<17:33, 10.98s/it]\u001b[A\n",
      " 12%|█▏        | 13/108 [02:23<17:22, 10.97s/it]\u001b[A\n",
      " 13%|█▎        | 14/108 [02:34<17:10, 10.96s/it]\u001b[A\n",
      " 14%|█▍        | 15/108 [02:45<17:00, 10.97s/it]\u001b[A\n",
      " 15%|█▍        | 16/108 [02:56<16:51, 10.99s/it]\u001b[A\n",
      " 16%|█▌        | 17/108 [03:07<16:41, 11.00s/it]\u001b[A\n",
      " 17%|█▋        | 18/108 [03:18<16:30, 11.01s/it]\u001b[A\n",
      " 18%|█▊        | 19/108 [03:29<16:18, 11.00s/it]\u001b[A\n",
      " 19%|█▊        | 20/108 [03:40<16:06, 10.98s/it]\u001b[A\n",
      " 19%|█▉        | 21/108 [03:51<15:54, 10.97s/it]\u001b[A\n",
      " 20%|██        | 22/108 [04:02<15:42, 10.96s/it]\u001b[A\n",
      " 21%|██▏       | 23/108 [04:13<15:32, 10.97s/it]\u001b[A\n",
      " 22%|██▏       | 24/108 [04:24<15:22, 10.99s/it]\u001b[A\n",
      " 23%|██▎       | 25/108 [04:35<15:12, 11.00s/it]\u001b[A\n",
      " 24%|██▍       | 26/108 [04:46<15:01, 11.00s/it]\u001b[A\n",
      " 25%|██▌       | 27/108 [04:57<14:50, 11.00s/it]\u001b[A\n",
      " 26%|██▌       | 28/108 [05:08<14:37, 10.97s/it]\u001b[A\n",
      " 27%|██▋       | 29/108 [05:19<14:25, 10.95s/it]\u001b[A\n",
      " 28%|██▊       | 30/108 [05:30<14:12, 10.93s/it]\u001b[A\n",
      " 29%|██▊       | 31/108 [05:41<14:03, 10.96s/it]\u001b[A\n",
      " 30%|██▉       | 32/108 [05:52<13:52, 10.96s/it]\u001b[A\n",
      " 31%|███       | 33/108 [06:03<13:42, 10.96s/it]\u001b[A\n",
      " 31%|███▏      | 34/108 [06:14<13:29, 10.94s/it]\u001b[A\n",
      " 32%|███▏      | 35/108 [06:24<13:18, 10.94s/it]\u001b[A\n",
      " 33%|███▎      | 36/108 [06:35<13:08, 10.95s/it]\u001b[A\n",
      " 34%|███▍      | 37/108 [06:46<12:56, 10.93s/it]\u001b[A\n",
      " 35%|███▌      | 38/108 [06:57<12:45, 10.93s/it]\u001b[A\n",
      " 36%|███▌      | 39/108 [07:08<12:35, 10.94s/it]\u001b[A\n",
      " 37%|███▋      | 40/108 [07:19<12:23, 10.93s/it]\u001b[A\n",
      " 38%|███▊      | 41/108 [07:30<12:12, 10.94s/it]\u001b[A\n",
      " 39%|███▉      | 42/108 [07:41<12:02, 10.95s/it]\u001b[A\n",
      " 40%|███▉      | 43/108 [07:52<11:50, 10.94s/it]\u001b[A\n",
      " 41%|████      | 44/108 [08:03<11:40, 10.95s/it]\u001b[A\n",
      " 42%|████▏     | 45/108 [08:14<11:29, 10.94s/it]\u001b[A\n",
      " 43%|████▎     | 46/108 [08:25<11:17, 10.92s/it]\u001b[A\n",
      " 44%|████▎     | 47/108 [08:36<11:05, 10.91s/it]\u001b[A\n",
      " 44%|████▍     | 48/108 [08:47<10:55, 10.93s/it]\u001b[A\n",
      " 45%|████▌     | 49/108 [08:57<10:44, 10.92s/it]\u001b[A\n",
      " 46%|████▋     | 50/108 [09:09<10:35, 10.96s/it]\u001b[A\n",
      " 47%|████▋     | 51/108 [09:19<10:23, 10.94s/it]\u001b[A\n",
      " 48%|████▊     | 52/108 [09:30<10:12, 10.93s/it]\u001b[A\n",
      " 49%|████▉     | 53/108 [09:41<10:02, 10.96s/it]\u001b[A\n",
      " 50%|█████     | 54/108 [09:52<09:50, 10.94s/it]\u001b[A\n",
      " 51%|█████     | 55/108 [10:03<09:40, 10.95s/it]\u001b[A\n",
      " 52%|█████▏    | 56/108 [10:14<09:28, 10.94s/it]\u001b[A\n",
      " 53%|█████▎    | 57/108 [10:25<09:17, 10.93s/it]\u001b[A\n",
      " 54%|█████▎    | 58/108 [10:36<09:07, 10.95s/it]\u001b[A\n",
      " 55%|█████▍    | 59/108 [10:47<08:58, 10.99s/it]\u001b[A\n",
      " 56%|█████▌    | 60/108 [10:58<08:46, 10.97s/it]\u001b[A\n",
      " 56%|█████▋    | 61/108 [11:09<08:34, 10.95s/it]\u001b[A\n",
      " 57%|█████▋    | 62/108 [11:20<08:23, 10.96s/it]\u001b[A\n",
      " 58%|█████▊    | 63/108 [11:31<08:12, 10.95s/it]\u001b[A\n",
      " 59%|█████▉    | 64/108 [11:42<08:01, 10.95s/it]\u001b[A\n",
      " 60%|██████    | 65/108 [11:53<07:50, 10.95s/it]\u001b[A\n",
      " 61%|██████    | 66/108 [12:04<07:39, 10.93s/it]\u001b[A\n",
      " 62%|██████▏   | 67/108 [12:15<07:28, 10.94s/it]\u001b[A\n",
      " 63%|██████▎   | 68/108 [12:26<07:17, 10.93s/it]\u001b[A\n",
      " 64%|██████▍   | 69/108 [12:36<07:05, 10.92s/it]\u001b[A\n",
      " 65%|██████▍   | 70/108 [12:47<06:55, 10.94s/it]\u001b[A\n",
      " 66%|██████▌   | 71/108 [12:58<06:44, 10.92s/it]\u001b[A\n",
      " 67%|██████▋   | 72/108 [13:09<06:32, 10.92s/it]\u001b[A\n",
      " 68%|██████▊   | 73/108 [13:20<06:22, 10.93s/it]\u001b[A\n",
      " 69%|██████▊   | 74/108 [13:31<06:11, 10.92s/it]\u001b[A\n",
      " 69%|██████▉   | 75/108 [13:42<06:00, 10.92s/it]\u001b[A\n",
      " 70%|███████   | 76/108 [13:53<05:49, 10.91s/it]\u001b[A\n",
      " 71%|███████▏  | 77/108 [14:04<05:38, 10.92s/it]\u001b[A\n",
      " 72%|███████▏  | 78/108 [14:15<05:27, 10.91s/it]\u001b[A\n",
      " 73%|███████▎  | 79/108 [14:26<05:17, 10.95s/it]\u001b[A\n",
      " 74%|███████▍  | 80/108 [14:37<05:06, 10.93s/it]\u001b[A\n",
      " 75%|███████▌  | 81/108 [14:48<04:54, 10.92s/it]\u001b[A\n",
      " 76%|███████▌  | 82/108 [14:58<04:44, 10.93s/it]\u001b[A\n",
      " 77%|███████▋  | 83/108 [15:09<04:33, 10.93s/it]\u001b[A\n",
      " 78%|███████▊  | 84/108 [15:20<04:22, 10.93s/it]\u001b[A\n",
      " 79%|███████▊  | 85/108 [15:31<04:11, 10.94s/it]\u001b[A\n",
      " 80%|███████▉  | 86/108 [15:42<04:00, 10.93s/it]\u001b[A\n",
      " 81%|████████  | 87/108 [15:53<03:49, 10.92s/it]\u001b[A\n",
      " 81%|████████▏ | 88/108 [16:04<03:38, 10.92s/it]\u001b[A\n",
      " 82%|████████▏ | 89/108 [16:15<03:27, 10.94s/it]\u001b[A\n",
      " 83%|████████▎ | 90/108 [16:26<03:16, 10.93s/it]\u001b[A\n",
      " 84%|████████▍ | 91/108 [16:37<03:05, 10.93s/it]\u001b[A\n",
      " 85%|████████▌ | 92/108 [16:48<02:55, 10.96s/it]\u001b[A\n",
      " 86%|████████▌ | 93/108 [16:59<02:44, 10.95s/it]\u001b[A\n",
      " 87%|████████▋ | 94/108 [17:10<02:34, 11.07s/it]\u001b[A\n",
      " 88%|████████▊ | 95/108 [17:21<02:23, 11.04s/it]\u001b[A\n",
      " 89%|████████▉ | 96/108 [17:32<02:12, 11.02s/it]\u001b[A\n",
      " 90%|████████▉ | 97/108 [17:43<02:00, 11.00s/it]\u001b[A\n",
      " 91%|█████████ | 98/108 [17:54<01:49, 10.99s/it]\u001b[A\n",
      " 92%|█████████▏| 99/108 [18:05<01:38, 10.98s/it]\u001b[A\n",
      " 93%|█████████▎| 100/108 [18:16<01:27, 10.97s/it]\u001b[A\n",
      " 94%|█████████▎| 101/108 [18:27<01:16, 10.99s/it]\u001b[A\n",
      " 94%|█████████▍| 102/108 [18:38<01:05, 10.98s/it]\u001b[A\n",
      " 95%|█████████▌| 103/108 [18:49<00:54, 10.97s/it]\u001b[A\n",
      " 96%|█████████▋| 104/108 [19:00<00:43, 10.96s/it]\u001b[A\n",
      " 97%|█████████▋| 105/108 [19:11<00:32, 10.97s/it]\u001b[A\n",
      " 98%|█████████▊| 106/108 [19:22<00:21, 10.97s/it]\u001b[A\n",
      " 99%|█████████▉| 107/108 [19:33<00:10, 10.97s/it]\u001b[A\n",
      "100%|██████████| 108/108 [19:44<00:00, 10.96s/it]\u001b[A\n",
      " 50%|█████     | 1/2 [19:44<19:44, 1184.16s/it]\n",
      "  0%|          | 0/108 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/108 [00:10<19:32, 10.95s/it]\u001b[A\n",
      "  2%|▏         | 2/108 [00:21<19:21, 10.95s/it]\u001b[A\n",
      "  3%|▎         | 3/108 [00:32<19:12, 10.98s/it]\u001b[A\n",
      "  4%|▎         | 4/108 [00:44<19:10, 11.06s/it]\u001b[A\n",
      "  5%|▍         | 5/108 [00:55<18:55, 11.02s/it]\u001b[A\n",
      "  6%|▌         | 6/108 [01:06<18:41, 11.00s/it]\u001b[A\n",
      "  6%|▋         | 7/108 [01:16<18:28, 10.98s/it]\u001b[A\n",
      "  7%|▋         | 8/108 [01:27<18:15, 10.96s/it]\u001b[A\n",
      "  8%|▊         | 9/108 [01:38<18:06, 10.97s/it]\u001b[A\n",
      "  9%|▉         | 10/108 [01:49<17:54, 10.97s/it]\u001b[A\n",
      " 10%|█         | 11/108 [02:00<17:44, 10.97s/it]\u001b[A\n",
      " 11%|█         | 12/108 [02:11<17:31, 10.96s/it]\u001b[A\n",
      " 12%|█▏        | 13/108 [02:22<17:18, 10.94s/it]\u001b[A\n",
      " 13%|█▎        | 14/108 [02:33<17:09, 10.95s/it]\u001b[A\n",
      " 14%|█▍        | 15/108 [02:44<16:56, 10.93s/it]\u001b[A\n",
      " 15%|█▍        | 16/108 [02:55<16:45, 10.92s/it]\u001b[A\n",
      " 16%|█▌        | 17/108 [03:06<16:34, 10.93s/it]\u001b[A\n",
      " 17%|█▋        | 18/108 [03:17<16:23, 10.93s/it]\u001b[A\n",
      " 18%|█▊        | 19/108 [03:28<16:13, 10.94s/it]\u001b[A\n",
      " 19%|█▊        | 20/108 [03:39<16:02, 10.94s/it]\u001b[A\n",
      " 19%|█▉        | 21/108 [03:50<15:53, 10.95s/it]\u001b[A\n",
      " 20%|██        | 22/108 [04:01<15:48, 11.03s/it]\u001b[A\n",
      " 21%|██▏       | 23/108 [04:12<15:35, 11.00s/it]\u001b[A\n",
      " 22%|██▏       | 24/108 [04:23<15:21, 10.98s/it]\u001b[A\n",
      " 23%|██▎       | 25/108 [04:34<15:11, 10.98s/it]\u001b[A\n",
      " 24%|██▍       | 26/108 [04:45<14:58, 10.96s/it]\u001b[A\n",
      " 25%|██▌       | 27/108 [04:56<14:46, 10.94s/it]\u001b[A\n",
      " 26%|██▌       | 28/108 [05:07<14:37, 10.97s/it]\u001b[A\n",
      " 27%|██▋       | 29/108 [05:17<14:25, 10.95s/it]\u001b[A\n",
      " 28%|██▊       | 30/108 [05:28<14:14, 10.95s/it]\u001b[A\n",
      " 29%|██▊       | 31/108 [05:39<14:05, 10.98s/it]\u001b[A\n",
      " 30%|██▉       | 32/108 [05:50<13:52, 10.95s/it]\u001b[A\n",
      " 31%|███       | 33/108 [06:01<13:42, 10.97s/it]\u001b[A\n",
      " 31%|███▏      | 34/108 [06:12<13:30, 10.95s/it]\u001b[A\n",
      " 32%|███▏      | 35/108 [06:23<13:18, 10.94s/it]\u001b[A\n",
      " 33%|███▎      | 36/108 [06:34<13:09, 10.96s/it]\u001b[A\n",
      " 34%|███▍      | 37/108 [06:45<12:58, 10.97s/it]\u001b[A\n",
      " 35%|███▌      | 38/108 [06:56<12:47, 10.97s/it]\u001b[A\n",
      " 36%|███▌      | 39/108 [07:07<12:35, 10.95s/it]\u001b[A\n",
      " 37%|███▋      | 40/108 [07:18<12:23, 10.94s/it]\u001b[A\n",
      " 38%|███▊      | 41/108 [07:29<12:12, 10.93s/it]\u001b[A\n",
      " 39%|███▉      | 42/108 [07:40<12:00, 10.92s/it]\u001b[A\n",
      " 40%|███▉      | 43/108 [07:51<11:48, 10.90s/it]\u001b[A\n",
      " 41%|████      | 44/108 [08:02<11:37, 10.90s/it]\u001b[A\n",
      " 42%|████▏     | 45/108 [08:12<11:26, 10.89s/it]\u001b[A\n",
      " 43%|████▎     | 46/108 [08:23<11:15, 10.89s/it]\u001b[A\n",
      " 44%|████▎     | 47/108 [08:34<11:04, 10.89s/it]\u001b[A\n",
      " 44%|████▍     | 48/108 [08:46<11:05, 11.10s/it]\u001b[A\n",
      " 45%|████▌     | 49/108 [08:57<11:04, 11.26s/it]\u001b[A\n",
      " 46%|████▋     | 50/108 [09:09<10:59, 11.37s/it]\u001b[A\n",
      " 47%|████▋     | 51/108 [09:21<10:52, 11.45s/it]\u001b[A\n",
      " 48%|████▊     | 52/108 [09:32<10:44, 11.50s/it]\u001b[A\n",
      " 49%|████▉     | 53/108 [09:44<10:34, 11.53s/it]\u001b[A\n",
      " 50%|█████     | 54/108 [09:55<10:24, 11.56s/it]\u001b[A\n",
      " 51%|█████     | 55/108 [10:07<10:13, 11.57s/it]\u001b[A\n",
      " 52%|█████▏    | 56/108 [10:18<09:56, 11.47s/it]\u001b[A\n",
      " 53%|█████▎    | 57/108 [10:30<09:41, 11.40s/it]\u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: 500 encountered ({\"error\":\"dial tcp 35.226.229.132:3307: connect: connection refused\"}), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:03:11.929028, resuming normal operation.\n",
      "\n",
      " 54%|█████▎    | 58/108 [10:40<09:22, 11.26s/it]\u001b[A\n",
      " 55%|█████▍    | 59/108 [10:51<09:06, 11.16s/it]\u001b[A\n",
      " 56%|█████▌    | 60/108 [11:02<08:52, 11.08s/it]\u001b[A\n",
      " 56%|█████▋    | 61/108 [11:13<08:38, 11.03s/it]\u001b[A\n",
      " 57%|█████▋    | 62/108 [11:24<08:25, 10.99s/it]\u001b[A\n",
      " 58%|█████▊    | 63/108 [11:35<08:13, 10.97s/it]\u001b[A\n",
      " 59%|█████▉    | 64/108 [11:46<08:05, 11.04s/it]\u001b[A\n",
      " 60%|██████    | 65/108 [11:58<07:58, 11.14s/it]\u001b[A\n",
      " 61%|██████    | 66/108 [12:09<07:53, 11.29s/it]\u001b[A\n",
      " 62%|██████▏   | 67/108 [12:20<07:38, 11.19s/it]\u001b[A\n",
      " 63%|██████▎   | 68/108 [12:31<07:24, 11.11s/it]\u001b[A\n",
      " 64%|██████▍   | 69/108 [12:42<07:11, 11.05s/it]\u001b[A\n",
      " 65%|██████▍   | 70/108 [12:53<06:58, 11.01s/it]\u001b[A\n",
      " 66%|██████▌   | 71/108 [13:04<06:46, 10.98s/it]\u001b[A\n",
      " 67%|██████▋   | 72/108 [13:15<06:34, 10.96s/it]\u001b[A\n",
      " 68%|██████▊   | 73/108 [13:26<06:23, 10.95s/it]\u001b[A\n",
      " 69%|██████▊   | 74/108 [13:37<06:12, 10.95s/it]\u001b[A\n",
      " 69%|██████▉   | 75/108 [13:48<06:00, 10.94s/it]\u001b[A\n",
      " 70%|███████   | 76/108 [13:58<05:49, 10.93s/it]\u001b[A\n",
      " 71%|███████▏  | 77/108 [14:09<05:38, 10.93s/it]\u001b[A\n",
      " 72%|███████▏  | 78/108 [14:20<05:27, 10.92s/it]\u001b[A\n",
      " 73%|███████▎  | 79/108 [14:31<05:17, 10.94s/it]\u001b[A\n",
      " 74%|███████▍  | 80/108 [14:42<05:07, 10.97s/it]\u001b[A\n",
      " 75%|███████▌  | 81/108 [14:53<04:55, 10.96s/it]\u001b[A\n",
      " 76%|███████▌  | 82/108 [15:04<04:45, 10.98s/it]\u001b[A\n",
      " 77%|███████▋  | 83/108 [15:15<04:33, 10.95s/it]\u001b[A\n",
      " 78%|███████▊  | 84/108 [15:26<04:22, 10.94s/it]\u001b[A\n",
      " 79%|███████▊  | 85/108 [15:37<04:11, 10.95s/it]\u001b[A\n",
      " 80%|███████▉  | 86/108 [15:48<04:00, 10.94s/it]\u001b[A\n",
      " 81%|████████  | 87/108 [15:59<03:49, 10.95s/it]\u001b[A\n",
      " 81%|████████▏ | 88/108 [16:10<03:38, 10.94s/it]\u001b[A\n",
      " 82%|████████▏ | 89/108 [16:21<03:27, 10.94s/it]\u001b[A\n",
      " 83%|████████▎ | 90/108 [16:32<03:16, 10.93s/it]\u001b[A\n",
      " 84%|████████▍ | 91/108 [16:43<03:06, 10.95s/it]\u001b[A\n",
      " 85%|████████▌ | 92/108 [16:54<02:55, 10.94s/it]\u001b[A\n",
      " 86%|████████▌ | 93/108 [17:05<02:43, 10.93s/it]\u001b[A\n",
      " 87%|████████▋ | 94/108 [17:16<02:33, 10.96s/it]\u001b[A\n",
      " 88%|████████▊ | 95/108 [17:26<02:22, 10.94s/it]\u001b[A\n",
      " 89%|████████▉ | 96/108 [17:37<02:11, 10.96s/it]\u001b[A\n",
      " 90%|████████▉ | 97/108 [17:48<02:00, 10.94s/it]\u001b[A\n",
      " 91%|█████████ | 98/108 [17:59<01:49, 10.92s/it]\u001b[A\n",
      " 92%|█████████▏| 99/108 [18:10<01:38, 10.95s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False, num_classes=50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "wandb.init(project=PROJECT_NAME,name=f'models.resnet18')\n",
    "for _ in tqdm(range(EPOCHS),leave=False):\n",
    "    for i in tqdm(range(0,len(X_train),BATCH_SIZE),leave=False):\n",
    "        X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "        y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "        model.to(device)\n",
    "        preds = model(X_batch)\n",
    "        preds = preds.to(device)\n",
    "        loss = criterion(preds,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({'loss':loss.item(),'val_loss':get_loss(criterion,y_test,model,X_test),'accuracy':test(model,X_train,y_train),'val_accuracy':test(model,X_test,y_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python373jvsc74a57bd0210f9608a45c0278a93c9e0b10db32a427986ab48cfc0d20c139811eb78c4bbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
